{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45d5472f",
   "metadata": {},
   "source": [
    "# Gender Classification using EfficientNet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2c4721",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800412de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "# PyTorch Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Evaluation Metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Albumentations for Augmentation\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226c1cba",
   "metadata": {},
   "source": [
    "## Step 2: Load & Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1468e9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for mounting\n",
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "#defining the paths of train and test folder\n",
    "# zip_path = \"/content/drive/MyDrive/Colab Notebooks/Comys_Hackathon5.zip\"\n",
    "# extract_path = \"/content/drive/MyDrive/Comys_Hackathon5\"\n",
    "\n",
    "# with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "#     zip_ref.extractall(extract_path)\n",
    "\n",
    "\n",
    "train_dir = \"/content/drive/MyDrive/Comys_Hackathon5/Comys_Hackathon5/Task_A/train\"\n",
    "test_dir = \"/content/drive/MyDrive/Comys_Hackathon5/Comys_Hackathon5/Task_A/val\"\n",
    "\n",
    "class AlbumentationsTransform:\n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, image):\n",
    "        image = np.array(image)\n",
    "        return self.transform(image=image)[\"image\"]\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "albumentations_train = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.CLAHE(p=0.3),\n",
    "    A.RandomShadow(p=0.3),\n",
    "    A.Normalize(mean=mean, std=std),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "albumentations_test = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize(mean=mean, std=std),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "train_transform = AlbumentationsTransform(albumentations_train)\n",
    "test_transform  = AlbumentationsTransform(albumentations_test)\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)\n",
    "test_dataset  = datasets.ImageFolder(test_dir, transform=test_transform)\n",
    "\n",
    "print(\"Classes:\", train_dataset.classes)\n",
    "print(\"Class-to-Index Mapping:\", train_dataset.class_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98df2e9c",
   "metadata": {},
   "source": [
    "## Step 3: Define Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f2d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = train_dataset.targets\n",
    "class_sample_count = np.array([np.sum(np.array(targets) == t) for t in np.unique(targets)])\n",
    "weights = 1. / class_sample_count\n",
    "sample_weights = np.array([weights[t] for t in targets])\n",
    "sample_weights = torch.from_numpy(sample_weights).double()\n",
    "\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, sampler=sampler)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "\n",
    "img, label = train_dataset[0]\n",
    "print(\"Sample Image Shape:\", img.shape)\n",
    "print(\"Sample Label:\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba3ef07",
   "metadata": {},
   "source": [
    "## Step 4: Model Setup (EfficientNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed554f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "!pip install -q efficientnet_pytorch\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "model._fc = nn.Sequential(\n",
    "    nn.Dropout(p=0.4),\n",
    "    nn.Linear(model._fc.in_features, 2)\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc47eb7a",
   "metadata": {},
   "source": [
    "## Step 5: Loss Function & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3095b4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "female_count = len(os.listdir(os.path.join(train_dir, \"female\")))\n",
    "male_count   = len(os.listdir(os.path.join(train_dir, \"male\")))\n",
    "total = female_count + male_count\n",
    "\n",
    "weights = [total / female_count, total / male_count]\n",
    "weights = torch.FloatTensor(weights).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d126024b",
   "metadata": {},
   "source": [
    "## Step 6: Training & Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cd54c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return total_loss / len(dataloader), accuracy_score(all_labels, all_preds), \\\n",
    "           precision_score(all_labels, all_preds), recall_score(all_labels, all_preds), f1_score(all_labels, all_preds)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return total_loss / len(dataloader), accuracy_score(all_labels, all_preds), \\\n",
    "           precision_score(all_labels, all_preds), recall_score(all_labels, all_preds), f1_score(all_labels, all_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10048555",
   "metadata": {},
   "source": [
    "## Step 7: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8b4dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "best_f1 = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    train_loss, train_acc, train_prec, train_rec, train_f1 = train_one_epoch(\n",
    "        model, train_loader, optimizer, criterion, device)\n",
    "\n",
    "    val_loss, val_acc, val_prec, val_rec, val_f1 = evaluate(\n",
    "        model, test_loader, criterion, device)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | Prec: {train_prec:.4f} | Rec: {train_rec:.4f} | F1: {train_f1:.4f}\")\n",
    "    print(f\"Val   Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | Prec: {val_prec:.4f} | Rec: {val_rec:.4f} | F1: {val_f1:.4f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        print(\"Best model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4490184d",
   "metadata": {},
   "source": [
    "## Step 8: Final Evaluation and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ae0c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "target_names = train_dataset.classes\n",
    "print(classification_report(all_labels, all_preds, target_names=target_names))\n",
    "\n",
    "import seaborn as sns\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a301be05",
   "metadata": {},
   "source": [
    "## Step 9: Save and Reload Model for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2490250",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"best_model.pth\")\n",
    "torch.save(model, \"gender_classification_model.pt\")\n",
    "\n",
    "model = EfficientNet.from_name('efficientnet-b0')\n",
    "model._fc = nn.Sequential(\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(model._fc.in_features, 2)\n",
    ")\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e936552",
   "metadata": {},
   "source": [
    "## Step 10: Inference on a New Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4022298",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transform = AlbumentationsTransform(albumentations_test)\n",
    "\n",
    "def predict_image(image_path, model, transform, class_names):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = np.array(image)\n",
    "    image = transform(image=image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        pred = torch.argmax(output, dim=1).item()\n",
    "\n",
    "    return class_names[pred]\n",
    "\n",
    "test_path = \"/content/drive/MyDrive/Comys_Hackathon5/Comys_Hackathon5/Task_A/train/male/Abraham_Foxman_0001.jpg\"\n",
    "prediction = predict_image(test_path, model, AlbumentationsTransform(albumentations_test), train_dataset.classes)\n",
    "print(f\"Predicted Gender: {prediction}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
